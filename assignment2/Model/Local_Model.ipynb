{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reading and sampling the data\n",
    "\n",
    "def read_file(path):\n",
    "    \"\"\"\n",
    "    reads the file in pandas df and converts the date_time column to datetime type\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    return df\n",
    "\n",
    "def sample_on_srch_id(df, frac = 0.1):\n",
    "    \"\"\"\n",
    "    samples the dataframe based on the fraction of srach_id\n",
    "    \"\"\"\n",
    "    # get unique srch_ids\n",
    "    srch_ids = np.unique(df.srch_id)\n",
    "    # calculate how many ids to return\n",
    "    chosen_k = int(len(srch_ids) * frac)\n",
    "    # sample ids\n",
    "    chosen_ids = random.sample(list(srch_ids), k = chosen_k)\n",
    "    # filter the df to only have sampled ids\n",
    "    return df[df['srch_id'].isin(chosen_ids)]\n",
    "\n",
    "### Feature Engineering --------------------------\n",
    "\n",
    "## missing data ----------------------------------\n",
    "\n",
    "def remove_missing_values(df):\n",
    "    \"\"\"\n",
    "    removes columns with more than 50 percent missing data\n",
    "    \"\"\"\n",
    "    missing_values = df.isna().mean().round(4) * 100\n",
    "    missing_values = pd.DataFrame(missing_values).reset_index()\n",
    "    missing_values.columns = [\"column\", \"missing\"]\n",
    "    # filter where there are missing values\n",
    "    missing_values.query(\"missing > 50\", inplace=True)  # remove columns with more than 50 % of missing values\n",
    "    missing_values.sort_values(\"missing\", inplace=True)\n",
    "    #print(missing_values)\n",
    "    df.drop(missing_values.column, axis=1, inplace=True)\n",
    "\n",
    "def replace_missing_values(df):\n",
    "    \"\"\"\n",
    "    imputes missing values with -1\n",
    "    \"\"\"\n",
    "    df.fillna(value=-1, inplace=True) \n",
    "\n",
    "## new features ----------------------------------\n",
    "\n",
    "def extract_time(df):\n",
    "    \"\"\" \n",
    "    month, week, day of the week and hour of search\n",
    "    \"\"\"\n",
    "    df_datetime = pd.DatetimeIndex(df.date_time)\n",
    "    df[\"month\"] = df_datetime.month\n",
    "    df[\"week\"] = df_datetime.week\n",
    "    df[\"day\"] = df_datetime.dayofweek + 1\n",
    "    df[\"hour\"] = df_datetime.hour\n",
    "    del df['date_time']\n",
    "\n",
    "def new_historical_price(df):\n",
    "    \"\"\"\n",
    "    'unlogs' prop_log_historical_price column\n",
    "    \"\"\"\n",
    "    df[\"prop_historical_price\"] = (np.e ** df.prop_log_historical_price).replace(1.0, 0)\n",
    "    df.drop(\"prop_log_historical_price\", axis=1, inplace=True)\n",
    "\n",
    "def add_price_position(df, rank_type = \"dense\"):\n",
    "    \"\"\"\n",
    "    adds hotel price position (\"price_position\") inside \"srch_id\" column\n",
    "    \"\"\"\n",
    "    ranks = df.groupby('srch_id')['price_usd'].rank(ascending=True, method = rank_type)\n",
    "    df[\"price_position\"] = ranks\n",
    "\n",
    "\n",
    "def average_numerical_features(df, group_by = [\"prop_id\"], columns = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_location_score2\"]):\n",
    "    \"\"\"\n",
    "    adds mean, median and standard deviation per prop_id (default) \n",
    "    for columns that are related to property (default)\n",
    "    \"\"\"\n",
    "    # caulcate means and rename columns\n",
    "    means = df.groupby(group_by)[columns].mean().reset_index()\n",
    "    means.columns = [means.columns[0]] + [x + \"_mean\" for x in means.columns[1:]]\n",
    "    # caulcate median and rename columns\n",
    "    medians = df.groupby(group_by)[columns].median().reset_index()\n",
    "    medians.columns = [medians.columns[0]] + [x + \"_median\" for x in medians.columns[1:]]\n",
    "    # caulcate means and rename columns\n",
    "    stds = df.groupby(group_by)[columns].std().reset_index()\n",
    "    stds.columns = [stds.columns[0]] + [x + \"_std\" for x in stds.columns[1:]]\n",
    "    ## attach aggregated data to the df\n",
    "    df = pd.merge(df, means, on=group_by)\n",
    "    df = pd.merge(df, medians, on=group_by)\n",
    "    df = pd.merge(df, stds, on=group_by)\n",
    "    return df\n",
    "\n",
    "def add_historical_booking_click(df):\n",
    "    \"\"\"\n",
    "    creates a column with the percentage of the prop_id booked/clicked rate overall\n",
    "    \"\"\"\n",
    "    # there are more prop_id in the test data than in train. \n",
    "    # Maybe we could still use this but would need to impute\n",
    "    # with the most common value (or something else)\n",
    "    \n",
    "    historical = df.groupby(\"prop_id\")[[\"click_bool\", \"booking_bool\"]].mean().reset_index()\n",
    "    historical.columns = [historical.columns[0]] + [x + \"_rate\" for x in historical.columns[1:]]\n",
    "    df = pd.merge(df, historical, on=\"prop_id\")\n",
    "    df.sort_values(\"srch_id\", inplace = True)\n",
    "    return df\n",
    "\n",
    "def join_historical_data(df, path = \"hist_click_book.csv\"):\n",
    "    \"\"\"\n",
    "    joins historical data according to prop_id. \n",
    "    path - location of historical data csv file\n",
    "    \n",
    "    \"\"\"\n",
    "    to_join = pd.read_csv(path)\n",
    "    joined = pd.merge(df, to_join, on=\"prop_id\")\n",
    "    return joined.sort_values(\"srch_id\")\n",
    "    \n",
    "    \n",
    "## other ----------------------------------\n",
    "\n",
    "def remove_cols(df, cols = [\"position\", \"prop_id\"]):\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "def remove_positions(df, positions = [5, 11, 17, 23]):\n",
    "    \"\"\"\n",
    "    removes hotels with specified positions \n",
    "    (based on the fact that hotels in those positions were not as booked)\n",
    "    \"\"\"\n",
    "    df = df[df[\"position\"].isin(positions) == False]\n",
    "\n",
    "def add_score(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    adds 'score' column to the df: 5 for booked, 1 for clicked\n",
    "    \"\"\"\n",
    "    \n",
    "    score = []\n",
    "    for book, click in zip(df.booking_bool, df.click_bool):\n",
    "        if book == 1:\n",
    "            score.append(5)\n",
    "            continue\n",
    "        if click == 1:\n",
    "            score.append(1)\n",
    "            continue\n",
    "        else:\n",
    "            score.append(0)\n",
    "    df[\"score\"] = score\n",
    "    del df['booking_bool']\n",
    "    del df['click_bool']\n",
    "\n",
    "def onehot(df, cols):\n",
    "    \"\"\" \n",
    "    returns a df with one-hot encoded columns (cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.get_dummies(df, columns=cols)\n",
    "\n",
    "def sampling(df, target, method=\"undersampling\", frac=0.3):\n",
    "    \n",
    "    \"\"\"\n",
    "    df: input dataframe\n",
    "    targetcol: target column of majority class\n",
    "    method: specifies method of sampling - 'undersampling' or 'combination' of undersampling and oversampling.\n",
    "    frac: final fraction minority wrt majority class (default fraction 0.15/0.85)\n",
    "    \n",
    "    returns: df with undersampled majority and oversampled minority class\n",
    "    \n",
    "    note that this only has to be performed on the training data!\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Split df in minority and majority\n",
    "    minority = df.loc[df[target] > 0]\n",
    "    majority = df.loc[df[target] == 0]\n",
    "    lenmin = len(minority)\n",
    "    lenmaj = len(majority)\n",
    "    \n",
    "    # Calculate current fraction\n",
    "    frac_min = lenmin/(lenmin+lenmaj)\n",
    "    frac_maj = 1-frac_min\n",
    "    print(f\"Current fraction:\\nMinority class: {frac_min}, Majority class: {frac_maj}\")\n",
    "    \n",
    "    if method == \"undersampling\":\n",
    "        \n",
    "        sampling_frac = ((1-frac)/frac*lenmin)/lenmaj\n",
    "        sampled_df = df.groupby('srch_id').sample(frac=sampling_frac)\n",
    "        fin_frac = lenmin / (len(sampled_df) + lenmin)\n",
    "\n",
    "    elif method == \"combination\":\n",
    "        \n",
    "        # This still needs to be implemented\n",
    "        \n",
    "        return\n",
    "    else:\n",
    "        raise ExceptionError(\"Invalid argument for 'method'\")\n",
    "        \n",
    "    \n",
    "    print(f\"Final fraction:\\nMinority class: {fin_frac}, Majority class: {1-fin_frac}\")\n",
    "    \n",
    "    dfs = [minority, sampled_df]\n",
    "    finaldf = pd.concat(dfs)\n",
    "    finaldf.sort_values(\"srch_id\", inplace = True)\n",
    "    finaldf = finaldf.reset_index(drop=True)\n",
    "\n",
    "    print(\"Done\")\n",
    "    return finaldf\n",
    "\n",
    "\n",
    "### Feature engineering function -----------\n",
    "\n",
    "def feature_engineering_train(df):\n",
    "    \n",
    "    extract_time(df)\n",
    "    remove_missing_values(df)\n",
    "    replace_missing_values(df)\n",
    "    new_historical_price(df)\n",
    "    add_price_position(df)\n",
    "#     df = average_numerical_features(df)\n",
    "    df = add_historical_booking_click(df)\n",
    "    add_score(df)\n",
    "    #remove_cols(df)\n",
    "    return df\n",
    "\n",
    "def feature_engineering_test(df):\n",
    "    \n",
    "    extract_time(df)\n",
    "    remove_missing_values(df)\n",
    "    replace_missing_values(df)\n",
    "    new_historical_price(df)\n",
    "    add_price_position(df)\n",
    "#     df = average_numerical_features(df)\n",
    "#     df = join_historical_data(df, path=\"data/hist_click_book.csv\")\n",
    "    return df\n",
    "    \n",
    "def create_df_queries_freq(df):\n",
    "    df_queries = pd.DataFrame()\n",
    "    df_queries = pd.crosstab(index=df['srch_id'], columns='count', colnames=['srch_id'])\n",
    "    df_queries.head()\n",
    "    df_queries.to_csv(\"../df_queries.csv\")\n",
    "    return pd.read_csv(\"../df_queries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/training_set_VU_DM.csv\")\n",
    "# testset = pd.read_csv(\"data/test_set_VU_DM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-b92790de89ba>:55: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  df[\"week\"] = df_datetime.week\n"
     ]
    }
   ],
   "source": [
    "# perform the feature engineering on the training data\n",
    "\n",
    "df = feature_engineering_train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>prop_brand_bool</th>\n",
       "      <th>prop_location_score1</th>\n",
       "      <th>prop_location_score2</th>\n",
       "      <th>...</th>\n",
       "      <th>random_bool</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>prop_historical_price</th>\n",
       "      <th>price_position</th>\n",
       "      <th>click_bool_rate</th>\n",
       "      <th>booking_bool_rate</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>141.174964</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14076</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>111106</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>106.697742</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13444</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>111000</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>151.411304</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.023734</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12851</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>107872</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>84.774942</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.016863</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12321</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>97247</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>107.770073</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.022642</td>\n",
       "      <td>0.011321</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       srch_id  site_id  visitor_location_country_id  prop_country_id  \\\n",
       "0            1       12                          187              219   \n",
       "14076        1       12                          187              219   \n",
       "13444        1       12                          187              219   \n",
       "12851        1       12                          187              219   \n",
       "12321        1       12                          187              219   \n",
       "\n",
       "       prop_id  prop_starrating  prop_review_score  prop_brand_bool  \\\n",
       "0          893                3                3.5                1   \n",
       "14076   111106                3                2.5                1   \n",
       "13444   111000                3                4.5                1   \n",
       "12851   107872                2                3.5                1   \n",
       "12321    97247                2                3.5                1   \n",
       "\n",
       "       prop_location_score1  prop_location_score2  ...  random_bool  month  \\\n",
       "0                      2.83                0.0438  ...            1      4   \n",
       "14076                  0.69                0.0071  ...            1      4   \n",
       "13444                  2.20                0.0164  ...            1      4   \n",
       "12851                  2.71                0.0465  ...            1      4   \n",
       "12321                  2.83                0.0145  ...            1      4   \n",
       "\n",
       "       week  day  hour  prop_historical_price  price_position  \\\n",
       "0        14    4     8             141.174964             3.0   \n",
       "14076    14    4     8             106.697742             5.0   \n",
       "13444    14    4     8             151.411304            16.0   \n",
       "12851    14    4     8              84.774942             7.0   \n",
       "12321    14    4     8             107.770073             4.0   \n",
       "\n",
       "       click_bool_rate  booking_bool_rate  score  \n",
       "0             0.026144           0.016340      0  \n",
       "14076         0.017391           0.008696      0  \n",
       "13444         0.023734           0.015823      0  \n",
       "12851         0.016863           0.008432      0  \n",
       "12321         0.022642           0.011321      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if everything is valid\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical, Important and Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['site_id', 'visitor_location_country_id', 'prop_country_id', 'day', 'hour', 'price_position', \n",
    "               'prop_starrating', 'price_usd', 'prop_historical_price', 'booking_bool_rate', 'click_bool_rate', \n",
    "               'srch_booking_window', 'day', 'hour', 'week', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important features for xgb model also\n",
    "\n",
    "important = ['price_position', 'prop_starrating', 'price_usd', 'prop_historical_price', \n",
    "             'booking_bool_rate', 'click_bool_rate', 'day', 'hour', 'week', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulting features\n",
    "\n",
    "numerical = list((set(df.columns) - set(categorical)) - set(['score', 'srch_id', 'position'])) + important\n",
    "numerical_test = list((set(df.columns) - set(categorical)) - set(['score', 'position'])) + important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET RF TRUE OR FALSE IF YOU WANT TO INCLUDE RANDOM FOREST \n",
    "# Useful for dealing with specifically categorical variables\n",
    "\n",
    "RF = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only useful for experimenting\n",
    "# Take a sample of 50 percent of all data\n",
    "\n",
    "subsample = 1\n",
    "\n",
    "df = sample_on_srch_id(df, frac = subsample)\n",
    "del df['position']\n",
    "\n",
    "# split sample in training- and test set by srch_id\n",
    "gss = GroupShuffleSplit(test_size=.52, n_splits=1, random_state = 7).split(df, groups=df['srch_id'])\n",
    "\n",
    "X_train_inds, X_test_inds = next(gss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= df.iloc[X_train_inds]\n",
    "\n",
    "# X_train and categorical X_train\n",
    "X_train = train_data.loc[:, ~train_data.columns.isin(['srch_id','score'])]\n",
    "\n",
    "if RF == \"True\":\n",
    "    c_X_train = X_train[categorical]\n",
    "    X_train = X_train[numerical]\n",
    "\n",
    "y_train = train_data.loc[:, train_data.columns.isin(['score'])]\n",
    "\n",
    "if RF == \"True\":\n",
    "    # replace 5 by 1 to make it binary classification\n",
    "    c_y_train = pd.DataFrame()\n",
    "    c_y_train['score'] = y_train['score'].replace(5, 1)\n",
    "\n",
    "del X_train['prop_id']\n",
    "\n",
    "groups = train_data.groupby('srch_id').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "test_data = df.iloc[X_test_inds]\n",
    "\n",
    "# X_test and categorical X_test\n",
    "X_test = test_data.loc[:, ~test_data.columns.isin(['score'])]\n",
    "\n",
    "if RF == \"True\":\n",
    "    c_X_test = X_test[categorical]\n",
    "    X_test = X_test[numerical_test]\n",
    "\n",
    "y_test = test_data.loc[:, test_data.columns.isin(['score'])]\n",
    "\n",
    "if RF == \"True\":\n",
    "    # replace 5 by 1 to make it binary classification\n",
    "    c_y_test = pd.DataFrame()\n",
    "    c_y_test['score'] = y_test['score'].replace(5, 1)\n",
    "\n",
    "# delete prop_id\n",
    "test_properties = X_test[\"prop_id\"].copy()\n",
    "del X_test['prop_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest for categorical variables: click probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest takes account of categorical values, therefore use a small model to train a model where the categorical values come to good right. This is not proper english I know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-cf14707fed2a>:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(c_X_train, c_y_train)\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn randomforest, train model with solely categorical features\n",
    "if RF == \"True\":\n",
    "    clf = RandomForestClassifier(n_estimators=45)\n",
    "    clf.fit(c_X_train, c_y_train)\n",
    "    y_pred_bool = clf.predict(c_X_test)\n",
    "\n",
    "    print(pd.crosstab(np.array(c_y_test).reshape(len(c_y_test,)), y_pred_bool))\n",
    "\n",
    "    y_pred = clf.predict_proba(c_X_test)\n",
    "\n",
    "    # Predicted probabilities\n",
    "    pp = [item[1] for item in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost: LambdaMART Pairwise Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRanker(  \n",
    "    tree_method='hist',\n",
    "    booster='gbtree',\n",
    "    objective='rank:pairwise',\n",
    "    random_state=42, \n",
    "    learning_rate=0.3,\n",
    "    colsample_bytree=0.9, \n",
    "    eta=0.05, \n",
    "    max_depth=6, \n",
    "    n_estimators=150, \n",
    "    subsample=0.75,\n",
    "    )\n",
    "\n",
    "model.fit(X_train, y_train, group=groups, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, df):\n",
    "    return model.predict(df.loc[:, ~df.columns.isin(['srch_id'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not necessary right?\n",
    "\n",
    "del test_data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (X_test.groupby('srch_id')\n",
    "               .apply(lambda x: predict(model, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most important features\n",
    "\n",
    "xgb.plot_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite RF and LambdaMART score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame()\n",
    "output[\"srch_id\"] = test_data[\"srch_id\"].copy()\n",
    "output[\"prop_id\"] = test_properties.copy()\n",
    "\n",
    "# Add scores\n",
    "pred_scores_list = []\n",
    "\n",
    "for i in predictions:\n",
    "    for j in i:\n",
    "        pred_scores_list.append(j)\n",
    "\n",
    "# Weight of RF model\n",
    "weigth = 0.5\n",
    "if RF == \"True\":\n",
    "    output[\"pred_scores\"] = [x + weigth*y for x, y in zip(pred_scores_list, pp)]\n",
    "else:\n",
    "    output[\"pred_scores\"] = pred_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort on predicted_score output within srch_id\n",
    "\n",
    "out = output.groupby('srch_id').apply(pd.DataFrame.sort_values, 'pred_scores', ascending=False)\n",
    "del out[\"pred_scores\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG(predictions, df, path_idcg = \"idcg.csv\"):\n",
    "    \"\"\"\n",
    "    takes predicted positions and calulates average ndcg.\n",
    "    predictions - dataframe must have \"srch_id\" and \"prop_id\" ordered by relevance (inside \"srch_id\") (basically Lotte's model \"out\" dataframe)\n",
    "    df - training dataset (must contain \"srch_id\", \"prop_id\", \"score\")\n",
    "    path_idcg - path to idcg scores per \"srch_id\"\n",
    "    \"\"\"\n",
    "    # reset index \n",
    "    predictions.reset_index(drop = True, inplace = True)\n",
    "    # add position + 1\n",
    "    predictions[\"position\"] = predictions.groupby(by = ['srch_id']).cumcount()+1\n",
    "    # filter to only have positions up to 5\n",
    "    predictions = predictions[predictions.position < 6]\n",
    "    # attach scores to predictions\n",
    "    predictions = pd.merge(predictions, df[[\"srch_id\", \"prop_id\", \"score\"]], on = [\"srch_id\", \"prop_id\"])\n",
    "    predictions[\"numerator\"] = predictions[\"score\"]\n",
    "    predictions[\"denominator\"] = np.log2(predictions[\"position\"])\n",
    "    predictions.loc[predictions.position == 1, \"denominator\"] = 1\n",
    "    predictions[\"intermediate_dcg\"] = predictions[\"numerator\"]/predictions[\"denominator\"]\n",
    "    dcg = predictions.groupby(\"srch_id\")[\"intermediate_dcg\"].sum().reset_index()\n",
    "    dcg.columns = [\"scrh_id\", \"DCG\"]\n",
    "    # read idcg\n",
    "    idcg = pd.read_csv(path_idcg)\n",
    "    # attach idcg to dcg\n",
    "    joined = pd.merge(dcg, idcg, on = \"scrh_id\")\n",
    "    # calculate NDCG\n",
    "    joined[\"NDCG\"] = joined[\"DCG\"]/joined[\"iDCG\"]\n",
    "    # calculate mean NDCG\n",
    "    return joined[\"NDCG\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reduced testdf for input in ndcg\n",
    "\n",
    "reduced_df = X_test.filter(['srch_id'], axis=1)\n",
    "reduced_df[\"prop_id\"] = test_properties\n",
    "reduced_df[\"score\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = NDCG(out, reduced_df, path_idcg = \"idcg.csv\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format output and Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Stop running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could be done more efficiently\n",
    "\n",
    "# Prepare output file\n",
    "output = pd.DataFrame()\n",
    "output[\"srch_id\"] = testset[\"srch_id\"]\n",
    "output[\"prop_id\"] = properties\n",
    "\n",
    "# Add scores\n",
    "pred_scores_list = []\n",
    "\n",
    "for i in predictions:\n",
    "    for j in i:\n",
    "        pred_scores_list.append(j)      \n",
    "\n",
    "output[\"pred_scores\"] = pred_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort on predicted_score output within srch_id\n",
    "\n",
    "out = output.groupby('srch_id').apply(pd.DataFrame.sort_values, 'pred_scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del out[\"pred_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "\n",
    "out.to_csv('data/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
